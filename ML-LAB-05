# ================= LAB 05 - Fixed =================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import os
import warnings

warnings.filterwarnings("ignore", category=FutureWarning)

# ---------- Functions ----------
def load_data(filepath):
if not os.path.exists(filepath):
raise FileNotFoundError(f"File not found: {filepath}")
return pd.read_csv(filepath)

def preprocess_data(df):
# Normalize column names
df.columns = df.columns.str.strip().str.lower()

# Candidate numeric features (only keep those present)
numeric_cols = ['midterm_score', 'final_score', 'assignments_avg',
'quizzes_avg', 'participation_score', 'projects_score',
'total_score', 'study_hours_per_week', 'attendance (%)',
'sleep_hours_per_night', 'stress_level (1-10)']
numeric_cols = [c for c in numeric_cols if c in df.columns]

# Convert numeric-like columns to numeric (coerce errors to NaN)
for c in numeric_cols:
df[c] = pd.to_numeric(df[c], errors='coerce')

return df, numeric_cols

def safe_mape(y_true, y_pred, eps=1e-9):
"""Mean Absolute Percentage Error that avoids division by zero by using eps."""
y_true = np.array(y_true, dtype=float)
y_pred = np.array(y_pred, dtype=float)
denom = np.where(np.abs(y_true) < eps, eps, y_true)
return np.mean(np.abs((y_true - y_pred) / denom)) * 100

def calc_regression_metrics(y_train, y_train_pred, y_test, y_test_pred):
def compute(y, y_pred):
mse = mean_squared_error(y, y_pred)
rmse = np.sqrt(mse)
mape = safe_mape(y, y_pred)
r2 = r2_score(y, y_pred)
return mse, rmse, mape, r2

return {
"train": compute(y_train, y_train_pred),
"test": compute(y_test, y_test_pred)
}

def regression_single_feature(df):
if 'midterm_score' not in df.columns or 'final_score' not in df.columns:
raise KeyError("Required columns 'midterm_score' and/or 'final_score' not found.")
X = df[['midterm_score']].copy()
y = df['final_score'].copy()

# Handle missing values: drop rows with missing target or feature
data = pd.concat([X, y], axis=1).dropna()
if data.shape[0] < 2:
raise ValueError("Not enough rows after dropping NaNs for regression.")

X_clean = data[['midterm_score']]
y_clean = data['final_score']

X_train, X_test, y_train, y_test = train_test_split(
X_clean, y_clean, test_size=0.3, random_state=42
)

model = LinearRegression().fit(X_train, y_train)
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

metrics = calc_regression_metrics(y_train, y_train_pred, y_test, y_test_pred)
return model, metrics

def regression_multi_feature(df, features):
if 'final_score' not in df.columns:
raise KeyError("Required column 'final_score' not found.")
if not features:
raise ValueError("No features provided for multi-feature regression.")

X = df[features].copy()
y = df['final_score'].copy()

# Fill numeric NaNs with column mean (simple strategy)
X = X.fillna(X.mean())
valid_idx = (~y.isna())
X = X.loc[valid_idx]
y = y.loc[valid_idx]

if X.shape[0] < 2:
raise ValueError("Not enough rows after cleaning for regression.")

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.3, random_state=42
)

model = LinearRegression().fit(X_train, y_train)
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

metrics = calc_regression_metrics(y_train, y_train_pred, y_test, y_test_pred)
return model, metrics

def perform_kmeans(df, features, k=2):
if not features:
raise ValueError("No features provided for clustering.")
X = df[features].copy()

# Fill NaNs with column mean and convert to numeric array
X = X.fillna(X.mean())
X_vals = X.values
if X_vals.shape[0] < k:
raise ValueError(f"Not enough samples ({X_vals.shape[0]}) for k={k} clusters.")

kmeans = KMeans(n_clusters=k, random_state=42, n_init=10).fit(X_vals)
labels = kmeans.labels_
centers = kmeans.cluster_centers_

# Compute clustering metrics (some metrics require >1 cluster and >1 sample per cluster)
silhouette = silhouette_score(X_vals, labels) if len(set(labels)) > 1 and X_vals.shape[0] > 1 else np.nan
ch_score = calinski_harabasz_score(X_vals, labels) if len(set(labels)) > 1 else np.nan
db_index = davies_bouldin_score(X_vals, labels) if len(set(labels)) > 1 else np.nan

return labels, centers, (silhouette, ch_score, db_index)

def clustering_scores_for_k(df, features, k_values=range(2,11)):
if not features:
raise ValueError("No features provided for clustering scores.")
X = df[features].copy().fillna(df[features].mean()).values
scores = {"k": [], "silhouette": [], "ch": [], "db": []}
for k in k_values:
if X.shape[0] < k:
# skip k values greater than number of samples
continue
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10).fit(X)
labels = kmeans.labels_
scores["k"].append(k)
scores["silhouette"].append(silhouette_score(X, labels) if len(set(labels)) > 1 else np.nan)
scores["ch"].append(calinski_harabasz_score(X, labels) if len(set(labels)) > 1 else np.nan)
scores["db"].append(davies_bouldin_score(X, labels) if len(set(labels)) > 1 else np.nan)
return scores

def elbow_plot(df, features, max_k=10):
if not features:
raise ValueError("No features provided for elbow plot.")
X = df[features].copy().fillna(df[features].mean()).values
distortions = []
k_range = range(2, min(max_k, X.shape[0]-1) + 1)
for k in k_range:
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10).fit(X)
distortions.append(kmeans.inertia_)
plt.plot(list(k_range), distortions, marker='o')
plt.xlabel("k")
plt.ylabel("Distortion (Inertia)")
plt.title("Elbow Method")
plt.show()

# ---------- Main ----------
if __name__ == "__main__":
try:
# Update this path to your dataset
filepath = "/Users/nikhilreddy/Desktop/Students Performance Dataset.csv"
df = load_data(filepath)

df, numeric_cols = preprocess_data(df)
if not numeric_cols:
raise ValueError("No numeric feature columns found. Check dataset column names.")

# A1 & A2: Regression with single feature
model1, metrics1 = regression_single_feature(df)
print("Single Feature Regression (midterm -> final):")
print("Train (MSE, RMSE, MAPE, R2):", metrics1["train"])
print("Test (MSE, RMSE, MAPE, R2):", metrics1["test"])

# A3: Regression with multiple features
# Use numeric_cols minus final_score as features
features = [c for c in numeric_cols if c != 'final_score']
if not features:
raise ValueError("No predictor features available for multi-feature regression.")
model2, metrics2 = regression_multi_feature(df, features)
print("\nMulti-Feature Regression (selected features -> final):")
print("Features used:", features)
print("Train (MSE, RMSE, MAPE, R2):", metrics2["train"])
print("Test (MSE, RMSE, MAPE, R2):", metrics2["test"])

# A4 & A5: KMeans clustering (k=2)
features_for_clustering = features.copy()
labels, centers, cluster_scores = perform_kmeans(df, features_for_clustering, k=2)
print("\nKMeans Clustering (k=2):")
print("Centers:\n", centers)
print("Scores (Silhouette, CH, DB):", cluster_scores)

# A6: Scores for multiple k
scores = clustering_scores_for_k(df, features_for_clustering, k_values=range(2, 8))
print("\nScores for different k values:")
for i in range(len(scores["k"])):
print(f"k={scores['k'][i]}, Sil={scores['silhouette'][i]:.3f}, CH={scores['ch'][i]:.3f}, DB={scores['db'][i]:.3f}")

# A7: Elbow Plot
elbow_plot(df, features_for_clustering, max_k=10)

except Exception as e:
print("Error:", str(e))
